---
layout: post
title: PATE and its influence
tags: [Privacy, Privacy-Preserving Machine Learning, Semi-Supervised Learning]
authors: Baweja, Prabhsimran, Carnegie Mellon University; Imran, Umaymah, Carnegie Mellon University; Naidu, Rakshit, Carnegie Mellon University
---

<!-- # This is a template file for a new blog post -->

# Introduction and Motivation

Privacy is a concern that usually arises in data-related fields. One such example would be in machine learning applications, where data is an integral component. 

Machine learning models require large amounts of data to learn from so that they can attain a reliable accuracy level. However, this training data may be sensitive in some applications, such as the medical histories of patients in a clinical trial. In an ideal scenario, the learning algorithm would guarantee to protect users’ privacy in a way that the model output isn’t able to re-identify a specific user from the data, but current machine learning algorithms don’t provide any such guarantees. In fact, it is possible for the model to implicitly and unknowingly memorize some of this training data such that through data analysis, it may be possible to identify specific users. This would in turn reveal the users’ sensitive information--thus making it difficult to shield individual privacy in the context of big data. Several papers have also presented examples of successful re-identification attacks on health data, for instance. 

In order to address this problem, the paper builds on specific techniques for knowledge aggregation and transfer with generative, semi-supervised methods. It describes an approach that can be applied generally, regardless of the details of the machine learning techniques, to provide strong privacy guarantees on the training data; it is called Private Aggregation of Teacher Ensembles (PATE). The main idea is to have a black box with multiple models called “teachers”, which would be trained on disjoint datasets. However, these models wouldn’t be published since they use potentially sensitive information. Therefore, a “student” model would instead learn to predict an output through a noisy voting between all the teacher models where only the topmost vote would be revealed. This way, the student model wouldn’t directly have access to the teacher model’s data or parameters. Moreover, the student model would have substantially quantified and bounded exposure to teachers’ knowledge. As a result, even if an attacker does get access to the internal workings of the student model, they wouldn’t be able to derive the sensitive information since the student model is training on a combination of the results from multiple teacher models. 

PATE’s idea can be extended to large datasets as well. It can apply to examples such as the problem of predicting customer ad clicks via machine learning, where both the number of users and the number of user features can be large. In this example, the user’s identity should be generalized and protected at the same time. So, PATE can be quite relevant to large, private datasets like these such that these datasets can be divided into smaller subsets. Next, these smaller subsets can be fed to the teacher models, which would train on them. Finally, the student model can be trained on the public data labeled by using the ensemble of the teacher models--thus leveraging the incomplete public data for private data analysis. 
Now that we’ve described the overall problem and general solution, we will moving into discussing the details of the methodology, experiments and conclusion.

# Methodology and Experiments


$$\begin{equation}
f(x) = arg max_j\{n_j(~x) + Lap\left(\dfrac{1}{\epsilon}\right)\}
\end{equation}$$

![PATE GIF](https://imgur.com/F5F5XDX)


# Conclusion
