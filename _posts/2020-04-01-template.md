---
layout: post
title: PATE and its influence in the Privacy-Preserving arena
tags: [Privacy, Privacy-Preserving Machine Learning, Semi-Supervised Learning]
authors: Baweja, Prabhsimran, Carnegie Mellon University; Imran, Umaymah, Carnegie Mellon University; Naidu, Rakshit, Carnegie Mellon University
---

<!-- # This is a template file for a new blog post -->

# Introduction and Motivation

Privacy is a concern that usually arises in data-related fields. One such example would be in machine learning applications, where data is an integral component [[Abadi et al., 2016]](#Abadi16). 

Machine learning models require large amounts of data to learn from so that they can attain a reliable accuracy level. However, this training data may be sensitive in some applications, such as the medical histories of patients in a clinical trial. In an ideal scenario, the learning algorithm would guarantee to protect users’ privacy in a way that the model output isn’t able to re-identify a specific user from the data, but current machine learning algorithms don’t provide any such guarantees. In fact, it is possible for the model to implicitly and unknowingly memorize some of this training data such that through data analysis, it may be possible to identify specific users. This would in turn reveal the users’ sensitive information--thus making it difficult to shield individual privacy in the context of big data. Several papers have also presented examples of successful re-identification attacks on health data, for instance. 

In order to address this problem, the paper builds on specific techniques for knowledge aggregation and transfer with generative, semi-supervised methods. It describes an approach that can be applied generally, regardless of the details of the machine learning techniques, to provide strong privacy guarantees on the training data; it is called Private Aggregation of Teacher Ensembles (PATE). The main idea is to have a black box with multiple models called “teachers”, which would be trained on disjoint datasets. However, these models wouldn’t be published since they use potentially sensitive information. Therefore, a “student” model would instead learn to predict an output through a noisy voting between all the teacher models where only the topmost vote would be revealed. This way, the student model wouldn’t directly have access to the teacher model’s data or parameters. Moreover, the student model would have substantially quantified and bounded exposure to teachers’ knowledge. As a result, even if an attacker does get access to the internal workings of the student model, they wouldn’t be able to derive the sensitive information since the student model is training on a combination of the results from multiple teacher models [[Papernot et al., 2017]](#Papernot17), [[Papernot et al., 2018]](#Papernot18). 

PATE’s idea can be extended to large datasets as well. It can apply to examples such as the problem of predicting customer ad clicks via machine learning, where both the number of users and the number of user features can be large. In this example, the user’s identity should be generalized and protected at the same time. So, PATE can be quite relevant to large, private datasets like these such that these datasets can be divided into smaller subsets. Next, these smaller subsets can be fed to the teacher models, which would train on them. Finally, the student model can be trained on the public data labeled by using the ensemble of the teacher models--thus leveraging the incomplete public data for private data analysis. 
Now that we’ve described the overall problem and general solution, we will moving into discussing the details of the methodology, experiments and conclusion.

# Methodology

After partitioning the data into $n$ teachers, we obtain $n$ classifiers which are called teacher models. These teacher models along with the provided incomplete public data, perform labeling using the teacher model predictions via aggregation. The labeling procedure is conducted by noisy voting where the noise added is either sampled from a Laplacian or Gaussian distribution. For the simplicity of this blog, we will restrict ourselves to noise sampled from Laplacian distribution. We formally define Differential Privacy (DP) below, 

Definition 1: Given a randomized mechanism $\mathcal{A}: \mathcal{D} \rightarrow \mathcal{R}$ (with domain $\mathcal{D}$ and range $\mathcal{R}$) and any two neighboring datasets $d_{1}, d_{2} \in \mathcal{D}$ (\emph{i.e.} they differ by a single individual data element), $\mathcal{A}$ is said to be $(\epsilon, \delta)$-differentially private for any subset $S \subseteq \mathcal{R}$

$$\begin{equation}
\Pr\left[ \mathcal{A}\left( d_{1}\right) \in S\right] \leq e^{\varepsilon }\cdot\Pr\left[ \mathcal{A}\left( d_{2}\right) \in S\right] + \delta
\end{equation}$$

Here, $\epsilon \geq 0, \delta \geq 0$. A $\delta = 0$ case corresponds to pure differential privacy, while both $\epsilon = 0, \delta = 0$ leads to an infinitely high privacy domain. Finally, $\epsilon = \infty$ provides no privacy guarantees. For practical purposes we want $\epsilon \leq 5, \delta \ll \frac{1}{N}$ where $N$ is the number of samples in the dataset. It is known that the Laplace distribution with location 0 and scale $\dfrac{1}{\epsilon}$, $Lap\left(0, \dfrac{1}{\epsilon}\right)\}$, follows $\epsilon$-DP [[Dwork and Roth, 2014]](#Dwork14).

The gif below explains the steps involved in PATE, 

![PATE GIF](https://imgur.com/a/6R1MwNk)

Step 1: Divide the dataset into n disjoint subsets.

Step 2: Train n different teacher models on the respective subset.

Step 3: Train the aggregated teacher model based on the outputs of the n teacher models.

Step 4: The predicted teacher is trained with Differential privacy by noisy voting count induced by both the incomplete public data.

Step 5: The student model is trained over the public data which is labeled by the ensemble.


The noisy voting procedure is computed as described below. We add

$$\begin{equation}
f(x) = arg max_j\{n_j(x) + Lap\left(\dfrac{1}{\epsilon}\right)\}
\end{equation}$$


# Discussion 





### References

<a name="Dwork14">Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3–4):211–407, 2014.</a>

<a name="Abadi16">M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proc. of the 2016 ACM SIGSAC Conf. on Computer and Communications Security (CCS’16), pages 308–318, 2016.</a>

<a name="Hamm16">Jihun Hamm, Yingjun Cao, and Mikhail Belkin. Learning privately from multiparty data. In International Conference on Machine Learning (ICML), pp. 555–563, 2016.</a>

<a name="Papernot17">Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semisupervised knowledge transfer for deep learning from private training data. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.</a>

<a name="Papernot18">Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson. Scalable private learning with pate. arXiv
preprint arXiv:1802.08908, 2018.</a>

<a name="Bagdas19">Bagdasaryan, E., Poursaeed, O., and Shmatikov, V. Differential privacy has disparate impact on model accuracy. In Advances in Neural Information Processing Systems, pp. 15479–15488, 2019 </a>

<a name="Uniyal21">A. Uniyal, R. Naidu, S. Kotti, S. Singh, P. J. Kenfack, F. Mireshghallah, and A. Trask. DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy? arXiv:2106.12576, 2021.<a>

<a name="Tran21">Cuong Tran, My H. Dinh, Kyle Beiter and Ferdinando Fioretto. A Fairness Analysis on Private Aggregation of Teacher Ensembles arXiv:2109.08630, 2021</a>

